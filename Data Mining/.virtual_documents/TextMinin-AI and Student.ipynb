# Sourcen
# https://www.youtube.com/watch?v=tjSxFAGP9Ss
# API KEY
# AIzaSyC3przSoI12WZ_gJiMIVnPX1Jr-MB-h4-8


import pandas as pd
import googleapiclient.discovery as gd
import matplotlib.pyplot as plt
from PIL import Image
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

import nltk # Library untuk preprocessing text
nltk.download('all',quiet=True) # download semua resource
pd.set_option('display.width',0)











def requestYoutubeComments(youtube, listComment, part, videoId, pageId = None):
    # Recursive function
    
    currentListComment = listComment
    if (pageId == None):
        # Make first request
        request = youtube.commentThreads().list(
            part=part,
            videoId=videoId
        )
    else:
        request = youtube.commentThreads().list(
            part=part,
            pageToken=pageId,
            videoId=videoId
        )
    rp = request.execute()

    newListComment = [{
            'publishedAt':data.get('snippet').get('topLevelComment').get('snippet').get('publishedAt'), # Mengambil waktu komentar
            'author':data.get('snippet').get('topLevelComment').get('snippet').get('authorDisplayName'), # Mengambil nama author
            'comment':data.get('snippet').get('topLevelComment').get('snippet').get('textOriginal'), # Mengambil isi komentar
        } for data in rp.get('items')]

    currentListComment.extend(newListComment)
    # Recursive function, jika tidak ada comment maka gunakan 
    if rp.get('nextPageToken') == None:
        return currentListComment
    else:
        return requestYoutubeComments(youtube, currentListComment, part, videoId,rp.get('nextPageToken'))

def getAllComments():

    # variable list menampung comment trheads
    listCommentThreads = []

    service_name = 'youtube'
    version = 'v3'
    API_KEY = 'AIzaSyC3przSoI12WZ_gJiMIVnPX1Jr-MB-h4-8'

    # Buat objek untuk membangun request
    youtube = gd.build(service_name, version,developerKey=API_KEY)

    # Request 1 return tjSxFAGP9Ss
    return requestYoutubeComments(youtube,[],'snippet,replies','hJP5GqnTrNo')

# Commented bia disimpan dulu saja datanya
# comments = getAllComments()

    





# df = pd.DataFrame.from_dict(comments)

df = pd.read_csv('Data/Ai_and_student/how_ai_could_save_students.csv')
df['real_index'] = df.index
df = df.drop(columns=['Unnamed: 0'])


df.head()


df.head()








df.comment.str.contains('sal|khan',regex=True, case=False)
df_first = pd.DataFrame(df.comment[False == df.comment.str.contains('sal|khan|Bhai',na=False,regex=True, case=False)])
df_filtered = pd.DataFrame(df_first.comment[df_first.comment.str.contains('ai|artficial intelligent|artificial intelligence', na=False ,regex=True, case=False)])
# Menambahkan index number
df_filtered['real_index'] = df_filtered.index

# Reindex
df_filtered = df_filtered.reset_index(drop=True)
df_filtered





from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer


contoh = 'Before ai art, making a living as an artist felt hopeless, now it feels pointless'
contoh_token = word_tokenize(contoh.lower())
print(contoh_token)

filtered_token = [token for token in contoh_token if token not in stopwords.words('english')]
print(filtered_token)

lemmatizer = WordNetLemmatizer()
lemmatized_token = [lemmatizer.lemmatize(token, pos='a') for token in filtered_token]
print(lemmatized_token)



def preprocessing_text(text):

    # print(type(text), text)
    # 1. Word tokenizatoin
    token = word_tokenize(text.lower())

    # 2. Remove stopword
    filter_token = [filteredToken for filteredToken in token if filteredToken not in stopwords.words('english')]

    # 3. Stemming (Merubah kalimat ke kalimat dasar)
    lemmatizer = WordNetLemmatizer()
    lemmatized = [lemmatizer.lemmatize(token) for token in filter_token]

    # Menggabungkan menjadi text kembali
    return ' '.join(lemmatized)


df_prepro = df_filtered
df_prepro.comment = df_prepro.comment.apply(lambda x : preprocessing_text(x))


df_final = df_prepro
df_prepro





from nltk.sentiment.vader import SentimentIntensityAnalyzer 


analyzer = SentimentIntensityAnalyzer()

def kalkulasi_sentiment(text):

    scores = analyzer.polarity_scores(text)

    if scores['compound'] > 0.05:
        return 1
    elif scores['compound'] < -0.5:
        return -1
    elif scores['compound'] >= -0.05 and scores['compound'] < 0.05:
        return 0
    


df_final['sentiment'] = df_final.comment.apply(lambda x : kalkulasi_sentiment(x))


df_final.to_csv('Data/Ai_and_student/clear_data_ai_saved_education.csv')


sentiment = df_final.groupby('sentiment').count()
sentiment[['comment']]





df_diaz = pd.DataFrame(df_prepro.comment[df_prepro.comment.str.contains('diaz|brahim|brahim diaz|diaz|brahim',regex=True)])
df_diaz = df_diaz.reset_index(drop=True)
df_diaz


df_luca = pd.DataFrame(df_prepro.comment[df_prepro.comment.str.contains('modriÄ‡|modric|luca|luka|modrik',regex=True)])
df_luca = df_luca.reset_index(drop=True)
df_luca





from nltk.sentiment.vader import SentimentIntensityAnalyzer 


analyzer = SentimentIntensityAnalyzer()

def kalkulasi_sentiment(text):

    scores = analyzer.polarity_scores(text)
    return 1 if scores['pos'] > 0 else 0
    


df_diaz['sentiment'] = df_diaz.comment.apply(lambda x : kalkulasi_sentiment(x))
df_luca['sentiment'] = df_luca.comment.apply(lambda x : kalkulasi_sentiment(x))


df_diaz





sentimentDiaz = df_diaz.groupby('sentiment').count()

print (f'''Penilaian negatif atau dalam arti menyalahkan Brahim Diaz atas penyebab kekalah pada final laga copa delrey Sebesar {(sentimentDiaz['comment'][0]*100)/(sentimentDiaz['comment'][0]+sentimentDiaz['comment'][1])} 
yang diambil dari {(sentimentDiaz['comment'][0]+sentimentDiaz['comment'][1])} komentar youtube yang mengadung nama brahim diaz''')


sentimentLuca = df_luca.groupby('sentiment').count()
print (f'''Penilaian negatif atau dalam arti menyalahkan Brahim Diaz atas penyebab kekalah pada final laga copa delrey Sebesar {(sentimentLuca['comment'][0]*100)/(sentimentLuca['comment'][0]+sentimentLuca['comment'][1])} 
yang diambil dari {(sentimentLuca['comment'][0]+sentimentLuca['comment'][1])} komentar youtube yang mengadung nama brahim diaz''')


sentimentDiaz


sentimentLuca





# Start with one review:
text = df_final.to_string(index=False)

# Create and generate a word cloud image:
wordcloud = WordCloud().generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()





# Start with one review:
text = df_final[df_final['sentiment'] == 1].to_string(index=False)

# Create and generate a word cloud image:
wordcloud = WordCloud().generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()





# Start with one review:
text = df_final[df_final['sentiment'] == 0].to_string(index=False)

# Create and generate a word cloud image:
wordcloud = WordCloud().generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()





# Start with one review:
text = df_final[df_final['sentiment'] == -1].to_string(index=False)

# Create and generate a word cloud image:
wordcloud = WordCloud().generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()


# Filter daftar koment sentiment positive
dfSentiment_Positive = df.iloc[df_final[df_final['sentiment'] == 1]['real_index'].to_list()]
dfSentiment_Positive


# Filter daftar koment sentiment netral
dfSentiment_Neutral = df.iloc[df_final[df_final['sentiment'] == 0]['real_index'].to_list()]
dfSentiment_Neutral


# Filter daftar koment sentiment negative
dfSentiment_Negative = df.iloc[df_final[df_final['sentiment'] == -1]['real_index'].to_list()]
dfSentiment_Negative


## Tulis ke CSV
dfSentiment_Positif.to_excel('Data/Ai_and_student/sentimenPositive.xlsx')
dfSentiment_Neutral.to_excel('Data/Ai_and_student/sentimenNeutral.xlsx')
dfSentiment_Negatif.to_excel('Data/Ai_and_student/sentimenNegative.xlsx')



