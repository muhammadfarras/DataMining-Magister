























import pandas as pd
import googleapiclient.discovery as gd
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
from PIL import Image
from bertopic import BERTopic
from IPython.display import clear_output
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import nltk # Library untuk preprocessing text
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
nltk.download('all',quiet=True) # download semua resource
pd.set_option('display.width',0)





def requestYoutubeComments(youtube, listComment, part, videoId, pageId = None):
    # Recursive function
    
    currentListComment = listComment
    if (pageId == None):
        # Make first request
        request = youtube.commentThreads().list(
            part=part,
            videoId=videoId
        )
    else:
        request = youtube.commentThreads().list(
            part=part,
            pageToken=pageId,
            videoId=videoId
        )
    rp = request.execute()

    newListComment = [{
            'publishedAt':data.get('snippet').get('topLevelComment').get('snippet').get('publishedAt'), # Mengambil waktu komentar
            'author':data.get('snippet').get('topLevelComment').get('snippet').get('authorDisplayName'), # Mengambil nama author
            'comment':data.get('snippet').get('topLevelComment').get('snippet').get('textOriginal'), # Mengambil isi komentar
        } for data in rp.get('items')]

    currentListComment.extend(newListComment)
    # Recursive function, jika tidak ada comment maka gunakan 
    if rp.get('nextPageToken') == None:
        return currentListComment
    else:
        return requestYoutubeComments(youtube, currentListComment, part, videoId,rp.get('nextPageToken'))

def getAllComments():

    # variable list menampung comment trheads
    listCommentThreads = []

    service_name = 'youtube'
    version = 'v3'
    API_KEY = 'AIzaSyC3przSoI12WZ_gJiMIVnPX1Jr-MB-h4-8'

    # Buat objek untuk membangun request
    youtube = gd.build(service_name, version,developerKey=API_KEY)

    # Request 1 return tjSxFAGP9Ss
    return requestYoutubeComments(youtube,[],'snippet,replies','5BDya7iCdmQ')

# Commented bia disimpan dulu saja datanya
comments = getAllComments()


df = pd.DataFrame.from_dict(comments)
df.head()





df.shape


df.comment.value_counts()





stopWordExtend = stopwords.words('indonesian')
extendList = ['yg','dsb','haha','ma','lo','mul','kau','kan','si','ga','lagi'] 
stopWordExtend.extend(extendList)


def preprocessing_text(text):

    # print(type(text), text)
    # 1. Word tokenizatoin
    token = word_tokenize(text.lower())

    # 2. Remove stopword
    filter_token = [filteredToken for filteredToken in token if filteredToken not in stopWordExtend]

    # 3. Stemming (Merubah kalimat ke kalimat dasar)
    lemmatizer = WordNetLemmatizer()
    lemmatized = [lemmatizer.lemmatize(token) for token in filter_token]

    # Menggabungkan menjadi text kembali
    return ' '.join(lemmatized)


df['comment'] =  df.comment.apply(lambda x : preprocessing_text(x))


# Konversi data frame kedalam bentuk list
docs = df.comment.to_list()





model = BERTopic(embedding_model="all-mpnet-base-v2").fit(docs) 
clear_output()





model.visualize_topics()


model.visualize_barchart()


model.visualize_heatmap()


from IPython.display import display
# Tampilkan semua kolom
pd.set_option('display.max_columns', None)

# Tampilkan semua baris
pd.set_option('display.max_rows', None)

# Jangan potong lebar kolom
pd.set_option('display.max_colwidth', None)

pd.set_option('display.width', 0)
model.get_topic_info()
# display(model.get_topic_info())






