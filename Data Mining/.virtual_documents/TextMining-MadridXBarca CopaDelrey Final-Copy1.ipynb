# Sourcen
# https://www.youtube.com/shorts/g3Z7279GC2I
# API KEY
# AIzaSyC3przSoI12WZ_gJiMIVnPX1Jr-MB-h4-8


import pandas as pd
import googleapiclient.discovery as gd
import matplotlib.pyplot as plt
from PIL import Image
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

import nltk # Library untuk preprocessing text
nltk.download('all',quiet=True) # download semua resource
pd.set_option('display.width',0)











def requestYoutubeComments(youtube, listComment, part, videoId, pageId = None):
    # Recursive function
    
    currentListComment = listComment
    if (pageId == None):
        # Make first request
        request = youtube.commentThreads().list(
            part=part,
            videoId=videoId
        )
    else:
        request = youtube.commentThreads().list(
            part=part,
            pageToken=pageId,
            videoId=videoId
        )
    rp = request.execute()

    newListComment = [{
            'publishedAt':data.get('snippet').get('topLevelComment').get('snippet').get('publishedAt'), # Mengambil waktu komentar
            'author':data.get('snippet').get('topLevelComment').get('snippet').get('authorDisplayName'), # Mengambil nama author
            'comment':data.get('snippet').get('topLevelComment').get('snippet').get('textOriginal'), # Mengambil isi komentar
        } for data in rp.get('items')]

    currentListComment.extend(newListComment)
    # Recursive function, jika tidak ada comment maka gunakan 
    if rp.get('nextPageToken') == None:
        return currentListComment
    else:
        return requestYoutubeComments(youtube, currentListComment, part, videoId,rp.get('nextPageToken'))

def getAllComments():

    # variable list menampung comment trheads
    listCommentThreads = []

    service_name = 'youtube'
    version = 'v3'
    API_KEY = 'AIzaSyC3przSoI12WZ_gJiMIVnPX1Jr-MB-h4-8'

    # Buat objek untuk membangun request
    youtube = gd.build(service_name, version,developerKey=API_KEY)

    # Request 1 return g3Z7279GC2I
    # Sample yg tidak ada video-nya 3iejRtbScIc
    return requestYoutubeComments(youtube,[],'snippet,replies','g3Z7279GC2I')

# Commented bia disimpan dulu saja datanya
comments = getAllComments()

    





df = pd.DataFrame.from_dict(comments)

df.to_csv('Data/komentar_luca_x_diaz.csv')


df.head()


df.shape








from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer


contoh = 'Modric don''t deserve that it was brahim that better so It was brahim fault'
contoh_token = word_tokenize(contoh.lower())
print(contoh_token)

filtered_token = [token for token in contoh_token if token not in stopwords.words('english')]
print(filtered_token)

lemmatizer = WordNetLemmatizer()
lemmatized_token = [lemmatizer.lemmatize(token, pos='a') for token in filtered_token]
print(lemmatized_token)



def preprocessing_text(text):

    # 1. Word tokenizatoin
    token = word_tokenize(text.lower())

    # 2. Remove stopword
    filter_token = [filteredToken for filteredToken in token if filteredToken not in stopwords.words('english')]

    # 3. Stemming (Merubah kalimat ke kalimat dasar)
    lemmatizer = WordNetLemmatizer()
    lemmatized = [lemmatizer.lemmatize(token) for token in filter_token]

    # Menggabungkan menjadi text kembali
    return ' '.join(lemmatized)


df_prepro = df
df_prepro.comment = df_prepro.comment.apply(lambda x : preprocessing_text(x))
df_prepro.head()








df_diaz = pd.DataFrame(df_prepro.comment[df_prepro.comment.str.contains('diaz|brahim|brahim diaz|diaz|brahim',regex=True)])
df_diaz = df_diaz.reset_index(drop=True)
df_diaz


df_luca = pd.DataFrame(df_prepro.comment[df_prepro.comment.str.contains('modriÄ‡|modric|luca|luka|modrik',regex=True)])
df_luca = df_luca.reset_index(drop=True)
df_luca





from nltk.sentiment.vader import SentimentIntensityAnalyzer 


analyzer = SentimentIntensityAnalyzer()

def kalkulasi_sentiment(text):

    scores = analyzer.polarity_scores(text)
    return 1 if scores['pos'] > 0 else 0
    


df_diaz['sentiment'] = df_diaz.comment.apply(lambda x : kalkulasi_sentiment(x))
df_luca['sentiment'] = df_luca.comment.apply(lambda x : kalkulasi_sentiment(x))


df_diaz





sentimentDiaz = df_diaz.groupby('sentiment').count()

print (f'''Penilaian negatif atau dalam arti menyalahkan Brahim Diaz atas penyebab kekalah pada final laga copa delrey Sebesar {(sentimentDiaz['comment'][0]*100)/(sentimentDiaz['comment'][0]+sentimentDiaz['comment'][1])} 
yang diambil dari {(sentimentDiaz['comment'][0]+sentimentDiaz['comment'][1])} komentar youtube yang mengadung nama brahim diaz''')


sentimentLuca = df_luca.groupby('sentiment').count()
print (f'''Penilaian negatif atau dalam arti menyalahkan Brahim Diaz atas penyebab kekalah pada final laga copa delrey Sebesar {(sentimentLuca['comment'][0]*100)/(sentimentLuca['comment'][0]+sentimentLuca['comment'][1])} 
yang diambil dari {(sentimentLuca['comment'][0]+sentimentLuca['comment'][1])} komentar youtube yang mengadung nama brahim diaz''')


sentimentDiaz


sentimentLuca





# Start with one review:
text = df_diaz[df_diaz['sentiment'] == 0].to_string(index=False)

# Create and generate a word cloud image:
wordcloud = WordCloud().generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()


# Start with one review:
text = df_luca[df_luca['sentiment'] == 0].to_string(index=False)

# Create and generate a word cloud image:
wordcloud = WordCloud().generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()





# Start with one review:
text = df_diaz[df_diaz['sentiment'] == 1].to_string(index=False)

# Create and generate a word cloud image:
wordcloud = WordCloud().generate(text)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()
