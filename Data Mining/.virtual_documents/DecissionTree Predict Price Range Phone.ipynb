import pandas as pd
import numpy as np
from pandas import DataFrame as df
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier # Please ultilize dir and help to find class or package
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix as cm
from plotly import graph_objects as go
# Setup pd
pd.options.display.max_columns = 30





def cmDisplay(confusion_matrix, judul=""):
    class_names=[0,1] # name  of classes
    fig, ax = plt.subplots()
    tick_marks = np.arange(len(class_names))
    plt.xticks(tick_marks, class_names)
    plt.yticks(tick_marks, class_names)
    # create heatmap
    sns.heatmap(pd.DataFrame(confusion_matrix), annot=True, cmap="YlGnBu" ,fmt='g')
    ax.xaxis.set_label_position("top")
    plt.tight_layout()
    plt.title('Confusion matrix '+judul, y=1.1)
    plt.ylabel('Actual label')
    plt.xlabel('Predicted label')

def tampilkan_score(y_train, y_test, y_pred_train, y_pred):
    # Evaluasi dari Model
    print ("\033[31mTrain Accuracy: " , metrics.accuracy_score (y_train , y_pred_train))
    print ("Test Accuracy: " , metrics.accuracy_score (y_test , y_pred))
    print ("F1 Score: ", f1_score (y_test , y_pred , average = "macro"))
    print ("Precision Score: ", precision_score (y_test , y_pred , average = "macro" ))
    print ("Recall Score: ", recall_score (y_test , y_pred , average = "macro" )) 
    print ('\n\33[33m',classification_report (y_test , y_pred))





raw_data = pd.read_csv("data/mobile_classification - Train.csv")
raw_data = raw_data.drop(columns=['Unnamed: 0'])
raw_data.head()


raw_data.shape


# Display data yg duplicate
raw_data[raw_data.duplicated()]


raw_data.isna().sum()


raw_data.info()








## Menampilan uniq value dari setiap kolom
data_diskrit = raw_data.nunique()[(raw_data.nunique() <= 8) & (raw_data.nunique() != 4)].index.to_list() ## Bilangan bulat
data_kontinue = raw_data.nunique()[(raw_data.nunique() > 8) & (raw_data.nunique() != 4)].index.to_list()

print(data_diskrit)
print(data_kontinue)


palette = ['#DEE0D5' ,'#A7C5C5' ,  '#E2AC48' , '#B96028' , '#983C2D']
# print(data_kontinue[10:])
#Ploting distribution and range of each column
plt.figure(figsize = (10,10))
plt.title("Rentang dan distribusi 1")
plt.xticks(rotation=45)
df_cont1 = raw_data[data_kontinue[0:5]]
sns.stripplot( data = df_cont1  , palette= palette )
plt.show()

#Ploting distribution and range of each column
plt.figure(figsize = (10,10))
plt.title("Rentang dan distribusi 2")
plt.xticks(rotation=45)
df_cont1 = raw_data[data_kontinue[5:10]]
sns.stripplot( data = df_cont1  , palette= palette )
plt.show()

palette = ['#DEE0D5' ,'#A7C5C5' ,  '#E2AC48']
#Ploting distribution and range of each column
plt.figure(figsize = (10,10))
plt.title("Rentang dan distribusi 3")
plt.xticks(rotation=45)
df_cont1 = raw_data[data_kontinue[10:]]
sns.stripplot( data = df_cont1  , palette= palette )
plt.show()


for a in data_diskrit:
    plt.figure(figsize=(10,1))
    sns.boxplot(x = raw_data[a], linewidth=1, showfliers=True)
    plt.xlabel(a, labelpad = 12)
    plt.show()
    print (a)





raw_data.describe()











raw_data['price_range'].unique()





X = raw_data.drop(columns=['price_range'])
y = raw_data[['price_range']]


X.shape, y.shape


y.head()





X_train, X_test, y_train, y_test = train_test_split(X,y, random_state = 42, test_size = 0.20)

print ('X Train',X_train.shape)
print ('X Test',X_test.shape)
print ('y Train',y_train.shape)
print ('y Test',y_test.shape)
model = DecisionTreeClassifier()
model.fit(X_train, y_train)


y_pred = classifier.predict (X_test) # predicting
y_pred_train = classifier.predict (X_train)
confusion_matrix = cm(y_test, y_pred)
confusion_matrix_train = cm(y_train, y_pred_train)

tampilkan_score(y_train, y_test, y_pred_train, y_pred)


cmDisplay(confusion_matrix_train,"Train")
print('\n'*2)
cmDisplay(confusion_matrix,"Test")








model_2 = DecisionTreeClassifier(criterion='entropy', max_depth = 9,random_state=0)

# random_stateint, RandomState instance or None, default=None
# Controls the randomness of the estimator. The features are always randomly permuted at each split, even if splitter is set to "best".
# When max_features < n_features, the algorithm will select max_features
# at random at each split before finding the best split among them.
# But the best found split may vary across different runs, even if max_features=n_features. 
# That is the case, if the improvement of the criterion is identical for several splits and one split has 
# to be selected at random. To obtain a deterministic behaviour during fitting, random_state has to be fixed to an integer. 
# See Glossary for details.


path = model_2.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas = path.ccp_alphas


## Menemukan cost complexity alpah
clfs = []
train_accuracy = []
test_accuracy = []
for n in ccp_alphas:
    model_best_alpha = DecisionTreeClassifier(criterion='entropy', max_depth = 9,random_state=0, ccp_alpha=n)
    model_best_alpha.fit(X_train, y_train)
    clfs.append(model_best_alpha)
    train_accuracy.append(model_best_alpha.score(X_train, y_train))
    test_accuracy.append(model_best_alpha.score(X_test, y_test))
    
print("Number of nodes in the last tree is: {} with ccp_alpha: {}".format(clfs[-1].tree_.node_count, ccp_alphas[-1]))

#plotting
fig = go.Figure()
fig.add_trace(go.Scatter(x=ccp_alphas, y=train_accuracy,
                    mode='lines+markers',
                    name='Train Accuracy',
                        line = dict(color='#E2AC48' , width=2)))
fig.add_trace(go.Scatter(x=ccp_alphas, y=test_accuracy,
                    mode='lines+markers',
                    name='Test Accuracy',
                        line = dict(color='#983C2D' , width=2)))
fig.update_layout(
    title='Train Accuracy vs Test Accuracy With Different CCPs',
    xaxis_title='Accuracy',
    yaxis_title='CCP',
    width=1000, 
    height=600,
    # plot_bgcolor='#DEE0D5'
)
cmap = ['#DEE0D5' ,'#A7C5C5' ,  '#E2AC48' , '#B96028' , ]
fig.show()


## Menemukan cost complexity alpha
clfs = []
train_accuracy = []
test_accuracy = []
avarage = []
ccp_alphas = ccp_alphas[ccp_alphas < 0.04]

for n in ccp_alphas:
    model_best_alpha = DecisionTreeClassifier(criterion='entropy', max_depth = 9,random_state=0, ccp_alpha=n)
    model_best_alpha.fit(X_train, y_train)
    clfs.append(model_best_alpha)
    train_accuracy.append(model_best_alpha.score(X_train, y_train))
    test_accuracy.append(model_best_alpha.score(X_test, y_test))
    avarage.append((model_best_alpha.score(X_train, y_train)+model_best_alpha.score(X_test, y_test))/2)
    
print("Number of nodes in the last tree is: {} with ccp_alpha: {}".format(clfs[-1].tree_.node_count, ccp_alphas[-1]))

#plotting
fig = go.Figure()
fig.add_trace(go.Scatter(x=ccp_alphas, y=train_accuracy,
                    mode='lines+markers',
                    name='Train Accuracy',
                        line = dict(color='#E2AC48' , width=2)))
fig.add_trace(go.Scatter(x=ccp_alphas, y=test_accuracy,
                    mode='lines+markers',
                    name='Test Accuracy',
                        line = dict(color='#983C2D' , width=2)))
fig.update_layout(
    title='Train Accuracy vs Test Accuracy With Different CCPs',
    xaxis_title='Accuracy',
    yaxis_title='CCP',
    width=1000, 
    height=600,
    # plot_bgcolor='#DEE0D5'
)
cmap = ['#DEE0D5' ,'#A7C5C5' ,  '#E2AC48' , '#B96028' , ]
fig.show()



