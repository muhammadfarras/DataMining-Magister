








# Import time
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
import matplotlib.pyplot as plt
from PIL import Image
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator





# df = pd.DataFrame.from_dict(comments)

df = pd.read_csv('Data/dummy_email_data.csv')
# df['real_index'] = df.index
# df = df.drop(columns=['Unnamed: 0'])
df.head()





def clean_text(text):
    """Doc cleaning"""
    
    # Lowering text
    text = text.lower()
    
    # Removing punctuation
    text = "".join([c for c in text if c not in PUNCTUATION])
    
    # Removing whitespace and newlines
    text = re.sub('\\s+',' ',text)
    
    return text

def sort_coo(coo_matrix):
    """Sort a dict with highest score"""
    tuples = zip(coo_matrix.col, coo_matrix.data)
    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)

def extract_topn_from_vector(feature_names, sorted_items, topn=10):
    """get the feature names and tf-idf score of top n items"""
    
    #use only topn items from vector
    sorted_items = sorted_items[:topn]

    score_vals = []
    feature_vals = []
    
    # word index and corresponding tf-idf score
    for idx, score in sorted_items:
        
        #keep track of feature name and its corresponding score
        score_vals.append(round(score, 3))
        feature_vals.append(feature_names[idx])

    #create a tuples of feature, score
    results= {}
    for idx in range(len(feature_vals)):
        results[feature_vals[idx]]=score_vals[idx]
    
    return results

def get_keywords(vectorizer, feature_names, doc):
    #generate tf-idf
    tf_idf_vector = vectorizer.transform([doc])
    
    #mengurutkan tf-idf vectors dengan descending order of scores
    sorted_items=sort_coo(tf_idf_vector.tocoo())

    #Hanya mengambil 10 kalimat paling sering muncul
    keywords=extract_topn_from_vector(feature_names,sorted_items,10)
    
    return list(keywords.keys())


# Static variable
PUNCTUATION = """!"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~"""


stopwords_list = stopwords.words('indonesian')


### Calculate


deskripsi = df['deskripsi email'].to_list()

vectorizer = TfidfVectorizer(stop_words=stopwords_list, smooth_idf=True, use_idf=True)# Creating vocab with our corpora
# Exlcluding first 10 docs for testing purpose
vectorizer.fit_transform(deskripsi)

# Storing vocab
feature_names = vectorizer.get_feature_names_out()


result = []
for doc in deskripsi:
    df = {}
    df['full_text'] = doc
    df['top_keywords'] = get_keywords(vectorizer, feature_names, doc)
    result.append(df)
    
final = pd.DataFrame(result)
final.index +=1
final


## Bags of words kalimat spam


topKeywordsSpam = final[final.index % 3 == 0].top_keywords
# Append datra
list_spam_top_list_words = [' '.join(a) for a in topKeywordsSpam]
string_spam_topList_words =' '.join(list_spam_top_list_words)
string_spam_topList_words


# Create and generate a word cloud image:
wordcloud = WordCloud().generate(string_spam_topList_words)

# Display the generated image:
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()



