








# Import time
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords





# df = pd.DataFrame.from_dict(comments)

df = pd.read_csv('Data/dummy_email_data.csv')
# df['real_index'] = df.index
# df = df.drop(columns=['Unnamed: 0'])
df.head()





def clean_text(text):
    """Doc cleaning"""
    
    # Lowering text
    text = text.lower()
    
    # Removing punctuation
    text = "".join([c for c in text if c not in PUNCTUATION])
    
    # Removing whitespace and newlines
    text = re.sub('\\s+',' ',text)
    
    return text



# Static variable
PUNCTUATION = """!"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~"""





### Calculate


deskripsi = df['deskripsi email'].to_list()

vectorizer = TfidfVectorizer(stop_words=stop_words.words('indonesian'), smooth_idf=True, use_idf=True)# Creating vocab with our corpora
# Exlcluding first 10 docs for testing purpose
vectorizer.fit_transform(deskripsi[10::])

# Storing vocab
feature_names = vectorizer.get_feature_names()



