# Import semua library yang dibutuh
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from pandas import DataFrame as df
import pandas as pd
from sklearn.model_selection import train_test_split

from sklearn import datasets
from torch.autograd import Variable



class LinearRegressionModel (nn.Module):
    def __init__(self, input_val, output_val):
        super(LinearRegressionModel, self).__init__()
        self.linear = torch.nn.Linear(input_val, output_val)

    def forward(self,x):
        y_pred = self.linear(x)
        return y_pred


model = LinearRegressionModel(1,1)
crieation = nn.MSELoss()
learning_rate = 0.01
optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)


X_numpy = np.arange(0,10)
X_numpy2 = np.arange(0,20,2)
# print(X_numpy.shape)
# Y_numpy = np.arange(0,20,2)
Y_numpy = ((2*X_numpy))

X_all = X_numpy.reshape(5,2)
X_all = np.concatenate((X_numpy, X_numpy2)).reshape(10,2)

print(X_all.shape)
# df({"X":X_numpy,"X2":X_numpy2,"Y":Y_numpy})

print("2X + 3Y")
df({'X':X_all[:,0],'X2':X_all[:,1],'Y':Y_numpy})


# generate the regression dataset
# X_numpy, y_numpy = datasets.make_regression(n_samples=150, n_features=1, noise=20, random_state=1)
print(X_numpy.shape)
# convert numpy array to pytorch tensor

# X = torch.from_numpy(X_numpy.astype(np.float32)) # also convert the values from double to float
X = torch.from_numpy(X_all.astype(np.float32)) # also convert the values from double to float

# print(X)
Y = torch.from_numpy(Y_numpy.astype(np.float32))
# reshape the y from 1d array to a column vector
Y = Y.view(y.shape[0], 1) # number of values is 1 and rows is 1
# print(y)
# print(X.shape)
# print(y.shape)
# Create the model, it's going to be just a single layer
# We use the built-in Linear model
n_samples, n_features = X.shape

input_size = n_features
print(input_size, output_size)
output_size = 1
model = nn.Linear(input_size, output_size)


# Define the loss and optimizer
learning_rate = 0.00001
loss_function = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # takes the model parameters and learning rate


# Create the training loop
epochs = 10_000
for epoch in range(epochs):
    
    # forward pass
    y_predicted = model(X)
    loss = loss_function(y_predicted, y) # takes the predicted values and actual values
    
    # backward pass: calculate gradients
    loss.backward()
    
    
    # perform weight updates
    optimizer.step()
    
    # empty the gradients before the next iterations so they don't accumulate
    optimizer.zero_grad()

    # print(epoch + 1 % 10)
    if (epoch + 1) % 100 == 0: # every 10 iterations
        # print(epoch)
        print(f"epoch: {epoch+1}, loss = {loss.item():4f}")
        
# prot the graphs
# We detach so that this operation is not tracked in the computational graph
predicted = model(X).detach().numpy() # detach the tensor and then convert to numpy array again.
plt.plot(X, Y, "ro")
# plt.plot(X, predicted, "b")
plt.show()





